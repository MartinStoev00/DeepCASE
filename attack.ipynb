{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T12:08:00.610776Z",
     "start_time": "2024-10-25T12:08:00.506570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from deepcase_copy.context_builder.loss import LabelSmoothing\n",
    "from deepcase_copy.context_builder.context_builder import ContextBuilder\n",
    "from deepcase_copy.interpreter.interpreter import Interpreter\n",
    "from deepcase_copy.interpreter.utils import group_by, sp_unique\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def disable_dropout(m):\n",
    "    if isinstance(m, torch.nn.Dropout):\n",
    "        m.p = 0.0 \n",
    "\n",
    "def to_cuda(item):\n",
    "    if torch.cuda.is_available():\n",
    "        return item.to('cuda')\n",
    "    return item\n",
    "\n",
    "def get_unique_indices_per_row(tensor):\n",
    "    indices_list = []\n",
    "    row_list = []\n",
    "    indices_list_set = set()\n",
    "    for row in range(len(tensor)):\n",
    "        curr = tuple(tensor[row])\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            curr = tuple(tensor[row].tolist())\n",
    "        if curr in indices_list_set:\n",
    "            continue\n",
    "        row_list.append(curr)\n",
    "        indices_list.append(row)\n",
    "        indices_list_set.add(curr)\n",
    "    return indices_list\n",
    "\n",
    "def get_unique_indices_per_row_or_file(tensor, f_name):\n",
    "    if os.path.exists(f_name):\n",
    "        return torch.load(f_name)\n",
    "    t = get_unique_indices_per_row(tensor)\n",
    "    torch.save(t, f_name)\n",
    "    return t  \n",
    "\n",
    "ALPHA=0.01\n",
    "EPSILON=0.5\n",
    "MAX_ITER = 100\n",
    "PERTURB_THRESHOLD = 10\n",
    "builder = to_cuda(ContextBuilder.load('save/save/builder.save'))\n",
    "builder.apply(disable_dropout)\n",
    "interpreter = Interpreter.load('save/save/interpreter.save', builder)\n",
    "criterion = LabelSmoothing(builder.decoder_event.out.out_features, 0.1) \n",
    "\n",
    "with open('save/save/sequences.save', 'rb') as infile:\n",
    "    data = torch.load(infile)\n",
    "    context = data[\"context\"] # 172572\n",
    "    events  = data[\"events\"]\n",
    "    labels  = data[\"labels\"]\n",
    "    \n",
    "    indices = get_unique_indices_per_row_or_file(context, 'save/save/context.pt')    \n",
    "    train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42, stratify=labels[indices])\n",
    "    \n",
    "    context_train = to_cuda(context[train_indices]) #\n",
    "    context_test  = to_cuda(context[test_indices])  # 4392\n",
    "    \n",
    "    events_train  = to_cuda(events[train_indices])\n",
    "    events_test   = to_cuda(events[test_indices])\n",
    "    \n",
    "    labels_train  = to_cuda(labels[train_indices])\n",
    "    labels_test   = to_cuda(labels[test_indices])\n",
    "    \n",
    "    clusters        = pd.read_csv('save/clusters.csv')['clusters'].values\n",
    "    clusters_train  = clusters[train_indices]\n",
    "    clusters_test   = clusters[test_indices]\n",
    "\n",
    "    prediction_label        = pd.read_csv('save/prediction.csv')['labels'].values\n",
    "    prediction_label_train  = prediction_label[train_indices]\n",
    "    prediction_label_test   = prediction_label[test_indices]\n",
    "\n",
    "def to_one_hot(t):\n",
    "    return builder.embedding_one_hot(t)\n",
    "\n",
    "def max_to_one(tensor):\n",
    "    max_indices = torch.argmax(tensor, dim=-1, keepdim=True)\n",
    "    result = torch.zeros_like(tensor)\n",
    "    result.scatter_(-1, max_indices, 1.0)\n",
    "    return result\n",
    "\n",
    "def get_results(results):\n",
    "    results_picked = torch.topk(results[0][0][0], 3)\n",
    "    exp = results_picked.values.exp()\n",
    "    res_indices = results_picked.indices\n",
    "    s = []\n",
    "    for j in range(3):\n",
    "        s.append(f\"{format_list([res_indices[j].item()])} {'{:.3f}'.format(exp[j])}\")\n",
    "    return res_indices, \", \".join(s)\n",
    "\n",
    "def compute_change(trace, original):\n",
    "    a = original - EPSILON\n",
    "    b = (trace >= a).float() * trace + (trace < a).float() * a\n",
    "    c = (b > original + EPSILON).float() * (original + EPSILON) + (b <= original + EPSILON).float() * b\n",
    "    return max_to_one(c), c\n",
    "\n",
    "def bim_attack(context_given, target_given):\n",
    "    change = None\n",
    "    original_context = to_cuda(to_one_hot(context_given).clone().detach())\n",
    "    context_processed = to_cuda(to_one_hot(context_given).clone().detach())\n",
    "    changes = []\n",
    "    computed_change_collected = []\n",
    "    for i in range(MAX_ITER):\n",
    "        context_processed.requires_grad_(True)\n",
    "        output = builder.predict(context_processed)\n",
    "        indices_of_results, prediction_str = get_results(output)\n",
    "        changes.append({\n",
    "            \"changed_to\": torch.argmax(context_processed, dim=-1).tolist()[0],\n",
    "            \"prediction_str\": prediction_str,\n",
    "        })\n",
    "        if target_given[0] != indices_of_results[0]:\n",
    "            break\n",
    "        loss = criterion(output[0][0], target_given)\n",
    "        context_processed.retain_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        grad = context_processed.grad.sign()\n",
    "        if change is None:\n",
    "            change = ALPHA * grad\n",
    "        else:\n",
    "            change += ALPHA * grad\n",
    "        context_processed = context_processed + change\n",
    "        context_processed, computed_change = compute_change(context_processed, original_context)\n",
    "        computed_change_collected.append(computed_change)\n",
    "    return changes, computed_change_collected\n",
    "\n",
    "def count_changes(changes_needed):\n",
    "    orig = changes_needed[0][\"changed_to\"]\n",
    "    final = changes_needed[-1][\"changed_to\"]\n",
    "    return count_list_diff(orig, final)\n",
    "\n",
    "def count_list_diff(orig, final):\n",
    "    changed_entries = 0\n",
    "    for orig, final in zip(orig, final):\n",
    "        if orig != final:\n",
    "            changed_entries += 1\n",
    "    return changed_entries\n",
    "    \n",
    "def show_changes(changes_needed):\n",
    "    orig = changes_needed[0][\"changed_to\"]\n",
    "    final = changes_needed[-1][\"changed_to\"]\n",
    "    changes = []\n",
    "    same = []\n",
    "    for orig, final in zip(orig, final):\n",
    "        if orig == final:\n",
    "            changes.append(\"-\")\n",
    "            same.append(final)\n",
    "        else:\n",
    "            changes.append(final)\n",
    "            same.append(\"XX\")\n",
    "    return format_list(changes), format_list(same)\n",
    "\n",
    "def format_list(li):\n",
    "    return f\"[{\", \".join([f'{num:2}' for num in li])}]\"\n",
    "            \n",
    "def format_line(current_trace_num, changes_needed, i):\n",
    "    start = \" \"*(len(str(current_trace_num)) + 2)\n",
    "    if i != 0:\n",
    "        if len(str(current_trace_num)) == 1:\n",
    "            start = f'{i:<3}'\n",
    "        elif len(str(current_trace_num)) == 2:\n",
    "            start = f'{i:<4}'\n",
    "        elif len(str(current_trace_num)) == 3:\n",
    "            start = f'{i:<5}'\n",
    "        elif len(str(current_trace_num)) == 4:\n",
    "            start = f'{i:<6}'\n",
    "        else:\n",
    "            start = f'{i:<7}'\n",
    "        b = list(start)\n",
    "        b[len(str(i))] = '<'\n",
    "        start = ''.join(b)\n",
    "    return f\"{start}{format_list(changes_needed[i][\"changed_to\"])} -> {changes_needed[i]['prediction_str']}\\n\"\n",
    "            \n",
    "def print_state(changes_needed, current_trace_num, con, event_chosen, change_collected, print_path=True, include_change=True):\n",
    "    mode_int = 0\n",
    "    changed_num = len(changes_needed)\n",
    "    perturbations_num = count_changes(changes_needed)\n",
    "    result_string = \"\"\n",
    "    if changed_num == 1:\n",
    "        pass\n",
    "    elif changed_num == MAX_ITER:\n",
    "        mode_int = 2\n",
    "    else:\n",
    "        mode_int = 1\n",
    "        result_string += f\"{current_trace_num}: {format_list(con[0].tolist())} == {event_chosen.tolist()} Changed {{{changed_num}}}, Perturbations {{{perturbations_num}}}\\n\"\n",
    "        if print_path:\n",
    "            for i in range(len(changes_needed)):\n",
    "                result_string += format_line(current_trace_num, changes_needed, i)                    \n",
    "                if include_change:\n",
    "                    if i is not len(changes_needed) - 1:\n",
    "                        result_string += f\"{\" \"*(len(str(current_trace_num)) + 2)} {change_collected[i]=}\\n\"\n",
    "        else:\n",
    "            for i in range(len(changes_needed)):\n",
    "                if i == 0 or (changes_needed[i][\"changed_to\"] != changes_needed[i-1][\"changed_to\"] and i != len(changes_needed) - 1):\n",
    "                    result_string += format_line(current_trace_num, changes_needed, i)\n",
    "            change_last = changes_needed[-1]\n",
    "            result_string += f\"{\" \"*(len(str(current_trace_num)) + 2)}{format_list(change_last[\"changed_to\"])} -> {change_last['prediction_str']}\\n\"\n",
    "        changed_entries, same_entries = show_changes(changes_needed)\n",
    "        result_string += f\"{\" \"*(len(str(current_trace_num)) - 1)}== {same_entries}\\n\"\n",
    "        result_string += f\"{\" \"*(len(str(current_trace_num)) - 1)}-> {changed_entries}\\n\"\n",
    "        result_string += \"\\n\" \n",
    "    return mode_int, result_string\n",
    "\n",
    "def process_traces(context_to_process, events_to_process, print_path=False, include_change=False, write_to_file=False, print_result=False):\n",
    "    f_name1 = f\"save/perturbed_collected/length={len(context_to_process)}, alpha={ALPHA}, epsilon={EPSILON}, num_iterations={MAX_ITER}, print_path={print_path} include_change={include_change}.pt\"\n",
    "    f_name2 = f\"save/perturbed_indices_collected/length={len(context_to_process)}, alpha={ALPHA}, epsilon={EPSILON}, num_iterations={MAX_ITER}, print_path={print_path} include_change={include_change}.pt\"\n",
    "    if os.path.exists(f_name1) and os.path.exists(f_name2):\n",
    "        print(f\"Loading {f_name1}\")\n",
    "        print(f\"Loading {f_name2}\")\n",
    "        with open(f\"results_trace/length={len(context_to_process)}, alpha={ALPHA}, epsilon={EPSILON}, num_iterations={MAX_ITER}, print_path={print_path} include_change={include_change}.txt\", 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            print(lines[-1])\n",
    "        return torch.load(f_name1), torch.load(f_name2)\n",
    "    length = len(context_to_process)\n",
    "    perturbed_collected_main = []\n",
    "    perturbed_indices = []\n",
    "    states = [0, 0, 0]\n",
    "    safe_to_file = \"\"\n",
    "    iters = range(length)\n",
    "    if not print_result:\n",
    "        iters = tqdm(iters)\n",
    "    for current_trace_num in iters:\n",
    "        con, e = context_to_process[current_trace_num], events_to_process.unsqueeze(1)[current_trace_num]\n",
    "        con.resize_(1, con.size()[-1])\n",
    "        changes_needed, change_collected = bim_attack(context_given=con, target_given=e)\n",
    "        mode_int, result_string = print_state(changes_needed, current_trace_num, con, e, change_collected, print_path=print_path, include_change=include_change)\n",
    "        if print_result:\n",
    "            print(result_string, end=\"\")\n",
    "        safe_to_file += result_string\n",
    "        if mode_int == 1:\n",
    "            perturbed_indices.append(current_trace_num)\n",
    "            perturbed_collected_main.append(changes_needed[-1]['changed_to'])\n",
    "        states[mode_int] += 1\n",
    "    print(f\"incorrect={states[0]} perturbed={states[1]} timeout={states[2]}\")\n",
    "    safe_to_file += f\"incorrect={states[0]} perturbed={states[1]} timeout={states[2]}\"\n",
    "    if write_to_file:\n",
    "        with open(f\"results_trace/length={length}, alpha={ALPHA}, epsilon={EPSILON}, num_iterations={MAX_ITER}, print_path={print_path} include_change={include_change}.txt\", \"w\") as f:\n",
    "            f.write(safe_to_file)\n",
    "    perturbed_collected_main = to_cuda(torch.tensor(perturbed_collected_main))\n",
    "    perturbed_indices = to_cuda(torch.tensor(perturbed_indices))\n",
    "    torch.save(perturbed_collected_main, f_name1)\n",
    "    torch.save(perturbed_indices, f_name2)\n",
    "    return perturbed_collected_main, perturbed_indices\n",
    "\n",
    "def inspect_index(index_inspected, context_l, events_l):\n",
    "    local_context_picked = to_cuda(context_l[index_inspected:index_inspected+1].clone().detach())\n",
    "    local_events_picked = to_cuda(events_l[index_inspected:index_inspected+1].clone().detach())\n",
    "    process_traces(local_context_picked, local_events_picked, print_path=True, include_change=False, write_to_file=True, print_result=True)\n",
    "    \n",
    "def get_changes_list(s, f):\n",
    "    perturbations_made = []\n",
    "    for i in range(len(s)):\n",
    "        if s[i] != f[i]:\n",
    "            perturbations_made.append((i, f[i]))\n",
    "            \n",
    "    return perturbations_made\n",
    "\n",
    "def get_possible_combinations(perturbations_made):\n",
    "    subsets = []\n",
    "    for r in range(1, len(perturbations_made) + 1):\n",
    "        subsets.extend(itertools.combinations(perturbations_made, r))\n",
    "    result = [list(subset) for subset in subsets]\n",
    "    return result\n",
    "\n",
    "def get_minimum_change_for_perturbation(start, final, events_picked):\n",
    "    combination_of_perturbation = get_possible_combinations(get_changes_list(start, final))\n",
    "    for i, combination in enumerate(combination_of_perturbation):\n",
    "        copy = to_cuda(start.clone().detach())\n",
    "        for index_of_change, value_of_change in combination:\n",
    "            copy[index_of_change] = value_of_change\n",
    "        copy.resize_(1, copy.size()[-1])\n",
    "        output = builder.predict(to_one_hot(copy))\n",
    "        indices_of_results, _ = get_results(output)\n",
    "        if events_picked != indices_of_results[0]:\n",
    "            if i != len(combination_of_perturbation) - 1:\n",
    "                if len(combination_of_perturbation) != 1:\n",
    "                    return copy, f\"Shortcut: {combination_of_perturbation[-1]} -> {combination}\\n\" \n",
    "    return final, \"\"\n",
    "\n",
    "def process_single(context_chosen):\n",
    "    result_string = \"\"\n",
    "    context_chosen = to_cuda(context_chosen.clone().detach())\n",
    "    output = predict_single(context_chosen)\n",
    "    attentions = [round(x, 5) for x in output[1][0][0].tolist()]\n",
    "    indices_of_results, prediction_str = get_results(output)\n",
    "    result_string += f\"{format_list(context_chosen[0].tolist())} -> {prediction_str}\\n\"\n",
    "    for c, a in zip(context_chosen[0], attentions):\n",
    "        result_string += f\"[{c:2}] {'{:.5f}'.format(a)} \"\n",
    "    result_string += \"\\n\"\n",
    "    return result_string\n",
    "    \n",
    "def predict_single(context_chosen):\n",
    "    context_chosen.resize_(1, context_chosen.size()[-1])\n",
    "    context_one_hot = to_one_hot(context_chosen)\n",
    "    return builder.predict(context_one_hot)\n",
    "\n",
    "def predict_single_from_list(list_picked, index_picked):\n",
    "    return predict_single(to_cuda(list_picked[index_picked:index_picked+1].detach()))\n",
    "\n",
    "def analysis(perturbed_chosen, context_chosen, events_chosen, index_chosen, index_inplace):\n",
    "    result_string = \"\"\n",
    "    start = context_chosen\n",
    "    final = perturbed_chosen\n",
    "    minimum_change_for_perturbation, final_combination = get_minimum_change_for_perturbation(start, final, events_chosen)\n",
    "    pick = final.clone().detach()\n",
    "    if minimum_change_for_perturbation is not None:\n",
    "        result_string += f\"Analyzing [{index_chosen}], Perturbations [{count_list_diff(start, final)}], Index [{index_inplace}], Changes [{get_changes_list(start, final)}]\"\n",
    "        if len(final_combination) != 0:\n",
    "            pick = minimum_change_for_perturbation.clone().detach()[0]\n",
    "            result_string += f\", Needed [{final_combination.split(\"->\")[1].count(\"(\")}]\"\n",
    "        result_string += f\"\\n{process_single(start)}{\"-\"*130}\\n\"\n",
    "        if len(final_combination) != 0:\n",
    "            result_string += process_single(final)\n",
    "            result_string += \"-\"*130 + \"\\n\"\n",
    "        result_string += process_single(minimum_change_for_perturbation)\n",
    "        result_string += final_combination\n",
    "    else:\n",
    "        result_string += f\"Change did not work on [{index_chosen}], Index [{index_inplace}]\\n\"\n",
    "    result_string += \"\\n\\n\"\n",
    "    return result_string, pick\n",
    "\n",
    "def find_shortcuts(perturbed_chosen, context_chosen, events_chosen, indices_chosen, print_details=False):\n",
    "    f_name = f\"save/perturbed_minimized/length={len(perturbed_chosen)}, alpha={ALPHA}, epsilon={EPSILON}, num_iterations={MAX_ITER}.pt\"\n",
    "    if os.path.exists(f_name):\n",
    "        print(f\"Loading {f_name}\")\n",
    "        print(f\"results_attention/length={len(perturbed_chosen)}, alpha={ALPHA}, epsilon={EPSILON}, num_iterations={MAX_ITER}.txt\")\n",
    "        with open(f\"results_attention/length={len(perturbed_chosen)}, alpha={ALPHA}, epsilon={EPSILON}, num_iterations={MAX_ITER}.txt\", 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            print(lines[-1])\n",
    "        return torch.load(f_name)\n",
    "    pick_list = []\n",
    "    with open(f\"results_attention/length={len(perturbed_chosen)}, alpha={ALPHA}, epsilon={EPSILON}, num_iterations={MAX_ITER}.txt\", \"w\") as file:\n",
    "        safe_to_file = \"\"\n",
    "        skipped = 0\n",
    "        shortcuts = 0\n",
    "        iters = enumerate(zip(perturbed_chosen, context_chosen, events_chosen, indices_chosen))\n",
    "        if not print_details:\n",
    "            iters = tqdm(iters, total=len(perturbed_chosen))\n",
    "        for i, perturbed_element  in iters:\n",
    "            r_string, pick = analysis(*perturbed_element, i)\n",
    "            pick_list.append(pick)\n",
    "            if \"Change did not work on\" in r_string:\n",
    "                skipped += 1\n",
    "            if \"Shortcut\" in r_string:\n",
    "                shortcuts += 1\n",
    "            if print_details:\n",
    "                print(r_string, end=\"\")\n",
    "            safe_to_file += r_string\n",
    "        skipped_str = f\"same={len(perturbed_chosen) - skipped - shortcuts} {shortcuts=} {skipped=}\"\n",
    "        print(skipped_str)\n",
    "        safe_to_file += skipped_str\n",
    "        file.write(safe_to_file)\n",
    "    pick_list = to_cuda(torch.stack(pick_list))\n",
    "    torch.save(pick_list, f_name)\n",
    "    return pick_list\n",
    "\n",
    "def sort_dist(trace_dist):\n",
    "    tensor_tuples = [tuple(t) if len(t) > 0 else (t.item(),) for t in trace_dist]\n",
    "    index_groups = {}\n",
    "    for idx, tensor_tuple in enumerate(tensor_tuples):\n",
    "        if tensor_tuple not in index_groups:\n",
    "            index_groups[tensor_tuple] = []\n",
    "        index_groups[tensor_tuple].append(idx)\n",
    "    sorted_groups = sorted(index_groups.items(), key=lambda x: (len(x[0]), x[0]))\n",
    "    return [idx for _, indices_l in sorted_groups for idx in indices_l]\n",
    "    \n",
    "def print_attention(x_mask, context_mask, vectors, neighbours, distance, scores):\n",
    "    data_collected = [] \n",
    "    for i in range(len(context_mask)):\n",
    "        l_value = torch.tensor(vectors[context_mask][i].toarray()[0])\n",
    "        l_indices = torch.nonzero(l_value, as_tuple=False).squeeze(1)\n",
    "        data_collected.append({\n",
    "            \"trace\": x_mask[context_mask][i],\n",
    "            \"indices\": l_indices.tolist(),\n",
    "            \"value\": l_value[l_indices].tolist(),\n",
    "            \"neighbour\": f\"{{{neighbours[i]:5}; {scores[i]} | {'{:.4f}'.format(distance[i][0]) }}}\"\n",
    "        })\n",
    "    res_str = \"\"\n",
    "    for s in sort_dist(list(map(lambda x: x[\"indices\"], data_collected))):\n",
    "        local_l = data_collected[s]\n",
    "        local_list = list(zip(local_l[\"indices\"], local_l[\"value\"]))\n",
    "        res = f\"{format_list(local_l[\"trace\"].tolist())} -> \"\n",
    "        for index, value in sorted(local_list, key=lambda x: x[1], reverse=True):\n",
    "            res += f\"[{f'{index:2}'}]: {'{:.4f}'.format(value)} \"\n",
    "        res_str += res + f\"{\" \"*(150 - len(res))} {local_l[\"neighbour\"]}\\n\"\n",
    "    return res_str\n",
    "\n",
    "def show_clusters(x, y):\n",
    "    vectors, mask = interpreter.attended_context(to_one_hot(x), y.reshape(-1, 1))\n",
    "    indices_y = group_by(y[mask].reshape(-1, 1).cpu().numpy(), lambda e: e.data.tobytes(),)\n",
    "    \n",
    "    with open(f\"result_cluster/length={len(x)}, alpha={ALPHA}, epsilon={EPSILON}, num_iterations={MAX_ITER}.txt\", \"w\") as file:\n",
    "        for event, context_mask in indices_y:\n",
    "            event = ord(event.decode('ascii')[0])   \n",
    "            if event not in interpreter.tree:\n",
    "                file.write(f\"{\"<\"*50}[{event}]{\">\"*50}\\n\")\n",
    "                continue\n",
    "            file.write(f\"{\"=\"*50}[{event}]{\"=\"*50}\\n\")\n",
    "            vectors_, inverse, _ = sp_unique(vectors[context_mask])\n",
    "            distance, neighbours = interpreter.tree[event].query(\n",
    "                X               = vectors_.toarray(),\n",
    "                return_distance = True,\n",
    "                dualtree        = vectors_.shape[0] >= 1e3, # Optimization\n",
    "            )\n",
    "            neighbours = interpreter.tree[event].get_arrays()[1][neighbours][:, 0]\n",
    "            scores = np.asarray([interpreter.labels[event][neighbour] for neighbour in neighbours])\n",
    "            file.write(print_attention(x[mask], context_mask, vectors, neighbours[inverse], distance[inverse], scores[inverse]) + \"\\n\")\n",
    "        \n",
    "def interpret(context_passed, events_passed, attention_query=True):\n",
    "    c = to_one_hot(context_passed)\n",
    "    e = events_passed.reshape(-1, 1)\n",
    "    # l = labels_test.cpu()    \n",
    "    # interpreter.cluster(c, e)\n",
    "    # scores = interpreter.score_clusters(l)\n",
    "    # interpreter.score(scores)\n",
    "    if not attention_query:\n",
    "        return interpreter.predict(X=c, y=e, iterations=0)\n",
    "    return interpreter.predict(X=c, y=e)"
   ],
   "id": "aed0a48282625435",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T11:35:36.443538Z",
     "start_time": "2024-10-25T11:35:34.657030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res_normal = interpret(context_test, events_test, attention_query=False)\n",
    "pd.Series(res_normal).value_counts().sort_index()"
   ],
   "id": "9080c2f76b7cefc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0    2675\n",
       "-1.0    1074\n",
       " 2.0     147\n",
       " 3.0     480\n",
       " 5.0      16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "55f74017dff0a986"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T11:57:38.128568Z",
     "start_time": "2024-10-25T11:57:38.122797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "perturbed_collected, perturbed_indices_collected = process_traces(\n",
    "    context_test, \n",
    "    events_test\n",
    ")  "
   ],
   "id": "833015caf1eaac91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading save/perturbed_collected/length=4392, alpha=0.01, epsilon=0.5, num_iterations=100, print_path=False include_change=False.pt\n",
      "Loading save/perturbed_indices_collected/length=4392, alpha=0.01, epsilon=0.5, num_iterations=100, print_path=False include_change=False.pt\n",
      "incorrect=1385 perturbed=2762 timeout=245\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T11:57:48.135802Z",
     "start_time": "2024-10-25T11:57:47.999723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res_perturbed_only = interpret(perturbed_collected, events_test[perturbed_indices_collected], attention_query=False)\n",
    "pd.Series(res_perturbed_only).value_counts().sort_index()"
   ],
   "id": "d122a2aa713022ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0      19\n",
       "-1.0    2743\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T11:35:36.970455Z",
     "start_time": "2024-10-25T11:35:36.580240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_test_original_perturbed = context_test.clone().detach()\n",
    "context_test_original_perturbed[perturbed_indices_collected] = perturbed_collected\n",
    "c_indices_perturbed = get_unique_indices_per_row(context_test_original_perturbed)\n",
    "res_combined_perturbed = interpret(context_test_original_perturbed[c_indices_perturbed], events_test[c_indices_perturbed], attention_query=False)\n",
    "pd.Series(res_combined_perturbed).value_counts().sort_index()"
   ],
   "id": "9363911580e357cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0     460\n",
       "-1.0    3219\n",
       " 2.0      20\n",
       " 3.0      97\n",
       " 5.0      14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T11:35:36.984999Z",
     "start_time": "2024-10-25T11:35:36.971508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "perturbed_minimized = find_shortcuts(\n",
    "    perturbed_collected, \n",
    "    context_test, \n",
    "    events_test,\n",
    "    perturbed_indices_collected\n",
    ")"
   ],
   "id": "c3abed99e82cd186",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading save/save/perturbed_minimized.pt\n",
      "results_attention/length=2762, alpha=0.01, epsilon=0.5, num_iterations=100.txt\n",
      "same=51 shortcuts=2711 skipped=0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T11:35:37.593888Z",
     "start_time": "2024-10-25T11:35:36.986305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res_perturbed_shortcuts = interpret(perturbed_minimized, events_test[perturbed_indices_collected], attention_query=True)\n",
    "pd.Series(res_perturbed_shortcuts).value_counts().sort_index()"
   ],
   "id": "2b48ebf26e550f02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0     281\n",
       "-1.0    2103\n",
       " 2.0       4\n",
       " 3.0     374\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T11:35:38.088149Z",
     "start_time": "2024-10-25T11:35:37.595070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_test_original_perturbed_shortcuts = context_test.clone().detach()\n",
    "context_test_original_perturbed_shortcuts[perturbed_indices_collected] = perturbed_minimized\n",
    "c_indices_perturbed_shortcuts = get_unique_indices_per_row(context_test_original_perturbed_shortcuts)\n",
    "res_combined_perturbed_shortcuts = interpret(context_test_original_perturbed_shortcuts[c_indices_perturbed_shortcuts], events_test[c_indices_perturbed_shortcuts], attention_query=False)\n",
    "pd.Series(res_combined_perturbed_shortcuts).value_counts().sort_index()"
   ],
   "id": "470009850f317626",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0     553\n",
       "-1.0    3686\n",
       " 2.0      26\n",
       " 3.0      98\n",
       " 5.0      14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T12:08:07.660751Z",
     "start_time": "2024-10-25T12:08:04.713751Z"
    }
   },
   "cell_type": "code",
   "source": "show_clusters(context_test, events_test)",
   "id": "212a9376d02c1f00",
   "outputs": [],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
