{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from deepcase_copy.context_builder.loss import LabelSmoothing\n",
    "from deepcase_copy.context_builder.context_builder import ContextBuilder\n",
    "from deepcase_copy.interpreter.interpreter import Interpreter\n",
    "from deepcase_copy.interpreter.utils import group_by, sp_unique\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from deepcase.context_builder.context_builder import ContextBuilder as BaseContextBuilder\n",
    "from deepcase.interpreter.interpreter import Interpreter as BaseInterpreter\n",
    "\n",
    "def disable_dropout(m):\n",
    "    if isinstance(m, torch.nn.Dropout):\n",
    "        m.p = 0.0 \n",
    "\n",
    "def to_cuda(item):\n",
    "    if torch.cuda.is_available():\n",
    "        return item.to('cuda')\n",
    "    return item\n",
    "\n",
    "def to_cuda_tensor(item):\n",
    "    return to_cuda(torch.tensor(item))\n",
    "\n",
    "def get_unique_indices_per_row(tensor):\n",
    "    indices_list = []\n",
    "    row_list = []\n",
    "    indices_list_set = set()\n",
    "    for row in tqdm(range(len(tensor))):\n",
    "        curr = tuple(tensor[row])\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            curr = tuple(tensor[row].tolist())\n",
    "        if curr in indices_list_set:\n",
    "            continue\n",
    "        row_list.append(curr)\n",
    "        indices_list.append(row)\n",
    "        indices_list_set.add(curr)\n",
    "    return indices_list\n",
    "\n",
    "def get_unique_indices_per_row_or_file(tensor, f_name):\n",
    "    if os.path.exists(f_name):\n",
    "        return torch.load(f_name)\n",
    "    t = get_unique_indices_per_row(tensor)\n",
    "    torch.save(t, f_name)\n",
    "    return t  \n",
    "\n",
    "SEQ_LEN=10\n",
    "MAX_ITER=20\n",
    "FEATURES=100\n",
    "DATASET=\"ided\"\n",
    "PICKING=\"rand\" if DATASET==\"hdfs\" else \"first\"\n",
    "PREDICT_THRESHOLD=0.2\n",
    "context_builder = to_cuda(ContextBuilder.load(f'{DATASET}/{SEQ_LEN=}/builder.save'))\n",
    "context_builder.apply(disable_dropout)\n",
    "interpreter = Interpreter.load(f'{DATASET}/{SEQ_LEN=}/interpreter.save', context_builder)\n",
    "interpreter.threshold = PREDICT_THRESHOLD\n",
    "criterion = LabelSmoothing(context_builder.decoder_event.out.out_features, 0.1)\n",
    "\n",
    "with open(f'{DATASET}/{SEQ_LEN=}/sequences.save', 'rb') as infile:\n",
    "    data = torch.load(infile)\n",
    "    context = data[\"context\"]\n",
    "    events  = data[\"events\"]\n",
    "    labels  = data[\"labels\"]\n",
    "    mapping = data[\"mapping\"]\n",
    "    \n",
    "    indices = get_unique_indices_per_row_or_file(context, f'{DATASET}/{SEQ_LEN=}/context.pt')\n",
    "    test_indices, labels_test = None, None\n",
    "    if labels is not None:\n",
    "        test_indices = train_test_split(indices, test_size=0.2, random_state=42, stratify=labels[indices])[1]\n",
    "        labels_test   = to_cuda(labels[test_indices])\n",
    "    else:\n",
    "       test_indices = train_test_split(indices, test_size=0.2, random_state=42)[1]\n",
    "    context_test  = to_cuda(context[test_indices])\n",
    "    events_test   = to_cuda(events[test_indices])\n",
    "\n",
    "def to_one_hot(t):\n",
    "    return to_cuda(context_builder.embedding_one_hot(t).clone().detach())\n",
    "\n",
    "def to_trace(o):\n",
    "    return torch.argmax(o, dim=-1).tolist()[0]\n",
    "\n",
    "def max_to_one_first(tensor):\n",
    "    max_indices = torch.argmax(tensor, dim=-1, keepdim=True)\n",
    "    result = torch.zeros_like(tensor)\n",
    "    result.scatter_(-1, max_indices, 1.0)\n",
    "    return result\n",
    "\n",
    "def max_to_one_rand(tensor):\n",
    "    max_values = torch.max(tensor.squeeze(0), dim=1).values\n",
    "    comparison = tensor == max_values.unsqueeze(1)\n",
    "    tensor_indices = torch.nonzero(comparison, as_tuple=True)\n",
    "    max_indices = [[] for _ in range(tensor.size(1))]\n",
    "    for row, col in zip(tensor_indices[1].tolist(), tensor_indices[2].tolist()):\n",
    "        max_indices[row].append(col)\n",
    "    random_max_indices = to_cuda_tensor([random.choice(sublist) for sublist in max_indices]).unsqueeze(0)\n",
    "    return to_one_hot(random_max_indices)\n",
    "\n",
    "def max_to_one(tensor):\n",
    "    return max_to_one_rand(tensor) if PICKING == \"rand\" else max_to_one_first(tensor)\n",
    "\n",
    "def to_output(context_chosen):\n",
    "    return context_builder.predict(context_chosen)\n",
    "\n",
    "def get_file_name(f_name, attention_query=False):\n",
    "    return f\"{DATASET}/{SEQ_LEN=}/{PREDICT_THRESHOLD=}/{PICKING}/{\"attention_query\" if attention_query else \"no_query\"}/{f_name}\"\n",
    "\n",
    "def get_correct_prediction_for_list(context_chosen, events_chosen, attention_query=False):\n",
    "    _, mask = interpreter.attended_context(\n",
    "        X           = to_one_hot(context_chosen),\n",
    "        y           = to_cuda(events_chosen),\n",
    "        iterations  = 100 if attention_query else 0\n",
    "    )\n",
    "    return torch.where(~mask)[0], torch.where(mask)[0]\n",
    "\n",
    "def get_performance(context_chosen, event_chosen, attention_query=False):\n",
    "    if attention_query:\n",
    "        context_processed = to_cuda_tensor(to_trace(context_chosen)).unsqueeze(0)\n",
    "        pred_true = get_correct_prediction_for_list(context_processed, event_chosen.unsqueeze(0).unsqueeze(0), attention_query=True)[1]\n",
    "        return len(pred_true) == 0\n",
    "    output = to_output(context_chosen)\n",
    "    results_picked = torch.topk(output[0][0][0], len(output[0][0][0]))\n",
    "    exp = results_picked.values.exp()\n",
    "    res_indices = results_picked.indices\n",
    "    for j in range(len(output[0][0][0])):\n",
    "        if res_indices[j].item() == event_chosen:\n",
    "            return exp[j] < PREDICT_THRESHOLD\n",
    "    return False\n",
    "\n",
    "def get_changes_list(start, final):\n",
    "    perturbations_made = []\n",
    "    for i, (s, f) in enumerate(zip(start, final)):\n",
    "        if s != f:\n",
    "            perturbations_made.append((i, f.item()))\n",
    "    return perturbations_made\n",
    "\n",
    "def get_context_builder_interpreter(train, path):\n",
    "    l_context_builder, l_interpreter = None, None\n",
    "    if os.path.exists(f'{path}/builder.save') and os.path.exists(f'{path}/interpreter.save'): \n",
    "        l_context_builder = ContextBuilder.load(f'{path}/builder.save')\n",
    "        l_interpreter = Interpreter.load(f'{path}/interpreter.save', l_context_builder)\n",
    "    else:\n",
    "        l_context_builder = BaseContextBuilder(\n",
    "            input_size=FEATURES,  # Number of input features to expect\n",
    "            output_size=FEATURES,  # Same as input size\n",
    "            hidden_size=128,  # Number of nodes in hidden layer, in paper we set this to 128\n",
    "            max_length=10,  # Length of the context, should be same as context in Preprocessor\n",
    "        )\n",
    "        l_context_builder.fit(\n",
    "            X             = context[train],               # Context to train with\n",
    "            y             = events[train].reshape(-1, 1), # Events to train with, note that these should be of shape=(n_events, 1)\n",
    "            epochs        = 10,                         # Number of epochs to train with\n",
    "            batch_size    = 128,                         # Number of samples in each training batch, in paper this was 128\n",
    "            learning_rate = 0.01,                        # Learning rate to train with, in paper this was 0.01\n",
    "            verbose       = True,                        # If True, prints progress\n",
    "        )\n",
    "        l_interpreter = BaseInterpreter(\n",
    "            context_builder = l_context_builder, # ContextBuilder used to fit data\n",
    "            features        = FEATURES,             # Number of input features to expect, should be same as ContextBuilder\n",
    "            eps             = 0.1,             # Epsilon value to use for DBSCAN clustering, in paper this was 0.1\n",
    "            min_samples     = 5,               # Minimum number of samples to use for DBSCAN clustering, in paper this was SEQ_LEN=5\n",
    "            threshold       = 0.2,             # Confidence threshold used for determining if attention from the ContextBuilder can be used, in paper this was 0.2\n",
    "        )\n",
    "        l_interpreter.cluster(\n",
    "            X          = context[train],               # Context to train with\n",
    "            y          = events[train].reshape(-1, 1), # Events to train with, note that these should be of shape=(n_events, 1)\n",
    "            iterations = 100,                         # Number of iterations to use for attention query, in paper this was 100\n",
    "            batch_size = 1024,                        # Batch size to use for attention query, used to limit CUDA memory usage\n",
    "            verbose    = True,                        # If True, prints progress\n",
    "        )\n",
    "        l_context_builder.save(f\"{path}/builder.save\")\n",
    "        l_interpreter.save(f\"{path}/interpreter.save\")\n",
    "    \n",
    "    l_context_builder = ContextBuilder.load(f'{path}/builder.save')\n",
    "    l_interpreter = Interpreter.load(f'{path}/interpreter.save', l_context_builder)\n",
    "    l_context_builder = to_cuda(l_context_builder)\n",
    "    l_context_builder.apply(disable_dropout)\n",
    "    return l_context_builder, l_interpreter, LabelSmoothing(l_context_builder.decoder_event.out.out_features, 0.1)\n",
    "\n",
    "def get_perturbations(context_chosen, event_chosen, attention_query=False):\n",
    "    perturbed_collected_main = []\n",
    "    perturbed_indices_main = []\n",
    "    perturbed_iterations_main = []\n",
    "    pred_false, pred_true = get_correct_prediction_for_list(context_chosen, event_chosen.unsqueeze(1), attention_query=attention_query)\n",
    "    states = [len(pred_false), 0, 0]\n",
    "    for current_trace_num in tqdm(pred_true):\n",
    "        con, e = context_chosen[current_trace_num].unsqueeze(0), event_chosen.unsqueeze(1)[current_trace_num]\n",
    "        perturbed_result, perturb_iterations = bim_attack(con, e, attention_query=attention_query)\n",
    "        if perturbed_result is not None:\n",
    "            states[1] += 1\n",
    "            perturbed_collected_main.append(perturbed_result)\n",
    "            perturbed_indices_main.append(current_trace_num)\n",
    "            perturbed_iterations_main.append(perturb_iterations)\n",
    "        else:\n",
    "            states[2] += 1\n",
    "    print(to_cuda_tensor(states))\n",
    "    return to_cuda_tensor(perturbed_collected_main), to_cuda_tensor(perturbed_indices_main), to_cuda_tensor(states), to_cuda_tensor(perturbed_iterations_main)\n",
    "\n",
    "def get_perturbations_or_location(context_chosen, event_chosen, attention_query=False):\n",
    "    path = f\"{DATASET}/events/{\"attention_query\" if attention_query else \"no_query\"}\"\n",
    "    f_name_perturbed = f\"{path}/perturbed_collected.pt\"\n",
    "    f_name_indices = f\"{path}/perturbed_indices.pt\"\n",
    "    f_name_distribution = f\"{path}/perturbed_distribution.pt\"\n",
    "    f_name_iterations = f\"{path}/perturbed_iterations.pt\"\n",
    "    if os.path.exists(f_name_perturbed) and os.path.exists(f_name_indices) and os.path.exists(f_name_distribution) and os.path.exists(f_name_iterations):\n",
    "        print(f\"Loading {f_name_perturbed}\")\n",
    "        print(f\"Loading {f_name_indices}\")\n",
    "        print(f\"Loading {f_name_distribution}\")\n",
    "        print(torch.load(f_name_distribution))\n",
    "        return torch.load(f_name_perturbed), torch.load(f_name_indices), torch.load(f_name_distribution), torch.load(f_name_iterations)\n",
    "    perturb_main, indices_main, result_main, iterations_main = get_perturbations(context_chosen, event_chosen, attention_query=attention_query)\n",
    "    torch.save(perturb_main, f_name_perturbed)\n",
    "    torch.save(indices_main, f_name_indices)\n",
    "    torch.save(result_main, f_name_distribution)\n",
    "    torch.save(iterations_main, f_name_iterations)\n",
    "    print(result_main)\n",
    "    return perturb_main, indices_main, result_main, iterations_main\n",
    "\n",
    "def get_perturbations_or_file(context_chosen, event_chosen, attention_query=False):\n",
    "    f_name_perturbed = get_file_name(\"perturbed_collected.pt\", attention_query=attention_query)\n",
    "    f_name_indices = get_file_name(\"perturbed_indices.pt\", attention_query=attention_query)\n",
    "    f_name_distribution = get_file_name(\"perturbed_distribution.pt\", attention_query=attention_query)\n",
    "    f_name_iterations = get_file_name(\"perturbed_iterations.pt\", attention_query=attention_query)\n",
    "    if os.path.exists(f_name_perturbed) and os.path.exists(f_name_indices) and os.path.exists(f_name_distribution) and os.path.exists(f_name_iterations):\n",
    "        print(f\"Loading {f_name_perturbed}\")\n",
    "        print(f\"Loading {f_name_indices}\")\n",
    "        print(f\"Loading {f_name_distribution}\")\n",
    "        print(torch.load(f_name_distribution))\n",
    "        return torch.load(f_name_perturbed), torch.load(f_name_indices), torch.load(f_name_distribution), torch.load(f_name_iterations)\n",
    "    os.makedirs(f\"{DATASET}/{SEQ_LEN=}/{PREDICT_THRESHOLD=}/{PICKING}/{MAX_ITER=}/{\"attention_query\" if attention_query else \"no_query\"}\", exist_ok=True)\n",
    "    perturb_main, indices_main, result_main, iterations_main = get_perturbations(context_chosen, event_chosen, attention_query=attention_query)\n",
    "    torch.save(perturb_main, f_name_perturbed)\n",
    "    torch.save(indices_main, f_name_indices)\n",
    "    torch.save(result_main, f_name_distribution)\n",
    "    torch.save(iterations_main, f_name_iterations)\n",
    "    print(result_main)\n",
    "    return perturb_main, indices_main, result_main, iterations_main\n",
    "\n",
    "def get_possible_combinations(perturbations_made):\n",
    "    subsets = []\n",
    "    for r_index in range(1, len(perturbations_made)):\n",
    "        subsets.extend(itertools.combinations(perturbations_made, r_index))\n",
    "    result = [list(subset) for subset in subsets]\n",
    "    return result\n",
    "\n",
    "def get_minimum_change_for_perturbation_no_query(perturbed_chosen, context_chosen, events_chosen):\n",
    "    for combination in get_possible_combinations(get_changes_list(context_chosen, perturbed_chosen)):\n",
    "        copy = to_cuda(context_chosen.clone().detach())\n",
    "        for index_of_change, value_of_change in combination:\n",
    "            copy[index_of_change] = value_of_change\n",
    "        if get_performance(to_one_hot(copy.unsqueeze(0)), events_chosen):\n",
    "            return copy\n",
    "    return perturbed_chosen\n",
    "\n",
    "def get_minimum_change_for_perturbation_attention_query(perturbed_chosen, context_chosen, events_chosen):\n",
    "    trace_combinations = []\n",
    "    for combination in get_possible_combinations(get_changes_list(context_chosen, perturbed_chosen)):\n",
    "        copy = to_cuda(context_chosen.clone().detach())\n",
    "        for index_of_change, value_of_change in combination:\n",
    "            copy[index_of_change] = value_of_change\n",
    "        trace_combinations.append(copy)\n",
    "    if len(trace_combinations) == 0:\n",
    "        return perturbed_chosen\n",
    "    mask_indices = get_correct_prediction_for_list(torch.stack(trace_combinations), torch.full((len(trace_combinations), 1), events_chosen.item()), True)[0]\n",
    "    return trace_combinations[mask_indices[0]] if len(mask_indices) != 0 else perturbed_chosen\n",
    "    \n",
    "def get_shortcuts(perturbed_chosen, context_chosen, events_chosen, attention_query=False):\n",
    "    get_shortcuts_func = get_minimum_change_for_perturbation_attention_query if attention_query else get_minimum_change_for_perturbation_no_query\n",
    "    pick_list = []\n",
    "    for perturbed_element in tqdm(zip(perturbed_chosen, context_chosen, events_chosen), total=len(perturbed_chosen)):\n",
    "        pick_list.append(get_shortcuts_func(*perturbed_element))\n",
    "    return to_cuda(torch.stack(pick_list))\n",
    "\n",
    "def get_shortcuts_or_file(perturbed_chosen, context_chosen, events_chosen, attention_query=False):\n",
    "    f_name = get_file_name(\"shortcuts.pt\", attention_query=attention_query)\n",
    "    if os.path.exists(f_name):\n",
    "        print(f\"Loading {f_name}\")\n",
    "        return torch.load(f_name)\n",
    "    pick_list = get_shortcuts(perturbed_chosen, context_chosen, events_chosen, attention_query)\n",
    "    torch.save(pick_list, f_name)\n",
    "    return pick_list\n",
    "\n",
    "def get_dist(trace_dist):\n",
    "    tensor_tuples = [tuple(t) if len(t) > 0 else (t.item(),) for t in trace_dist]\n",
    "    index_groups = {}\n",
    "    for idx, tensor_tuple in enumerate(tensor_tuples):\n",
    "        if tensor_tuple not in index_groups:\n",
    "            index_groups[tensor_tuple] = []\n",
    "        index_groups[tensor_tuple].append(idx)\n",
    "    sorted_groups = sorted(index_groups.items(), key=lambda x: (len(x[0]), x[0]))\n",
    "    return [idx for _, indices_l in sorted_groups for idx in indices_l]\n",
    "    \n",
    "def get_matrix_perturb(context_chosen, perturb_chosen):\n",
    "    matrix = [0] * SEQ_LEN\n",
    "    for c, p in zip(context_chosen, perturb_chosen):\n",
    "        cp = len(get_changes_list(c, p)) - 1\n",
    "        matrix[cp] += 1\n",
    "    return pd.DataFrame(matrix, index=range(1, SEQ_LEN + 1))\n",
    "\n",
    "def get_matrix_shortcuts(context_chosen, shortcut_chosen, perturb_chosen):\n",
    "    matrix = [[0] * SEQ_LEN for _ in range(SEQ_LEN)]\n",
    "    for c, s, p in zip(context_chosen, shortcut_chosen, perturb_chosen):\n",
    "        cs = len(get_changes_list(c, s)) - 1\n",
    "        cp = len(get_changes_list(c, p)) - 1\n",
    "        matrix[cp][cs] += 1\n",
    "    p_res = pd.DataFrame(matrix, index=range(1, SEQ_LEN + 1), columns=range(1, SEQ_LEN + 1))\n",
    "    return p_res.loc[:, p_res.sum() != 0]\n",
    "    \n",
    "def interpret_query(context_passed, events_passed):\n",
    "    c = to_one_hot(context_passed)\n",
    "    e = events_passed.reshape(-1, 1)\n",
    "    return interpreter.predict(X=c, y=e)\n",
    "\n",
    "def interpret(context_passed, events_passed):\n",
    "    c = to_one_hot(context_passed)\n",
    "    e = events_passed.reshape(-1, 1)\n",
    "    return interpreter.predict(X=c, y=e, iterations=0)\n",
    "    \n",
    "def get_combined(perturbed_chosen, perturbed_indices_chosen, attention_query=False):\n",
    "    interpret_func = interpret_query if attention_query else interpret\n",
    "    context_test_copy = context_test.clone().detach()\n",
    "    context_test_copy[perturbed_indices_chosen] = perturbed_chosen\n",
    "    # context_test_copy_indices = get_unique_indices_per_row(context_test_copy)\n",
    "    return interpret_func(context_test_copy, events_test)\n",
    "    \n",
    "def bim_attack(context_chosen, event_chosen, attention_query=False):\n",
    "    context_processed = to_one_hot(context_chosen)\n",
    "    for iteration in range(MAX_ITER):\n",
    "        context_processed.requires_grad_(True)\n",
    "        output = context_builder.predict(context_processed)\n",
    "        if get_performance(context_processed, event_chosen[0], attention_query=attention_query):\n",
    "            return to_trace(context_processed), iteration\n",
    "        loss = criterion(output[0][0], event_chosen)\n",
    "        context_processed.retain_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        context_processed = max_to_one(context_processed + context_processed.grad.sign())\n",
    "    return None, -1\n",
    "    \n",
    "def format_results(results):\n",
    "    results_picked = torch.topk(results[0][0][0], 3)\n",
    "    exp = results_picked.values.exp()\n",
    "    res_indices = results_picked.indices\n",
    "    return \", \".join([f\"{format_list([res_indices[j].item()])} {'{:.3f}'.format(exp[j])}\" for j in range(3)])\n",
    "  \n",
    "def format_changes(start, final):\n",
    "    changes = []\n",
    "    same = []\n",
    "    for s, f in zip(start, final):\n",
    "        if s == f:\n",
    "            changes.append(\"-\")\n",
    "            same.append(final)\n",
    "        else:\n",
    "            changes.append(f)\n",
    "            same.append(\"XX\")\n",
    "    return format_list(changes), format_list(same)\n",
    "\n",
    "def format_list(li):\n",
    "    return f\"[{\", \".join([f'{num:2}' for num in li])}]\"\n",
    "            \n",
    "def format_trace_prediction(current_trace_num, trace):\n",
    "    start = \" \"*(len(str(current_trace_num)) + 2)\n",
    "    trace = to_cuda(trace.clone().detach()).unsqueeze(0)\n",
    "    return f\"{start}{format_list(trace)} -> {format_results(to_output(trace))}\\n\"\n",
    "            \n",
    "def format_perturbation(perturb_chosen, perturb_int, context_chosen, event_chosen, current_trace_num):\n",
    "    result_string = f\"{current_trace_num}: {format_list(context_chosen[0].tolist())} == {event_chosen.tolist()} Changed [{perturb_int}], Perturbations [{len(get_changes_list(context_chosen, perturb_chosen))}]\\n\"\n",
    "    result_string += format_trace_prediction(current_trace_num, context_chosen)    \n",
    "    result_string += format_trace_prediction(current_trace_num, perturb_chosen)    \n",
    "    changed_entries, same_entries = format_changes(context_chosen, perturb_chosen)\n",
    "    result_string += f\"{\" \"*(len(str(current_trace_num)) - 1)}== {same_entries}\\n{\" \"*(len(str(current_trace_num)) - 1)}-> {changed_entries}\\n\\n\"\n",
    "    return result_string\n",
    "\n",
    "def format_attention(x_mask, context_mask, vectors, neighbours, distance, scores):\n",
    "    data_collected = [] \n",
    "    for i in range(len(context_mask)):\n",
    "        l_value = torch.tensor(vectors[context_mask][i].toarray()[0])\n",
    "        l_indices = torch.nonzero(l_value, as_tuple=False).squeeze(1)\n",
    "        data_collected.append({\n",
    "            \"trace\": x_mask[context_mask][i],\n",
    "            \"indices\": l_indices.tolist(),\n",
    "            \"value\": l_value[l_indices].tolist(),\n",
    "            \"neighbour\": f\"{{{neighbours[i]:5}; {scores[i]} | {'{:.4f}'.format(distance[i][0]) }}}\"\n",
    "        })\n",
    "    res_str = \"\"\n",
    "    for s in get_dist(list(map(lambda x: x[\"indices\"], data_collected))):\n",
    "        local_l = data_collected[s]\n",
    "        local_list = list(zip(local_l[\"indices\"], local_l[\"value\"]))\n",
    "        res = f\"{format_list(local_l[\"trace\"].tolist())} -> \"\n",
    "        for index, value in sorted(local_list, key=lambda x: x[1], reverse=True):\n",
    "            res += f\"[{f'{index:2}'}]: {'{:.4f}'.format(value)} \"\n",
    "        res_str += res + f\"{\" \"*(150 - len(res))} {local_l[\"neighbour\"]}\\n\"\n",
    "    return res_str\n",
    "\n",
    "def format_series(series):\n",
    "    return pd.Series(series).value_counts().sort_index()\n",
    "\n",
    "def format_confusion_matrix(y_true, y_pred):\n",
    "    format_labels = sorted(list(set(y_true.tolist()) | set(y_pred.tolist())))\n",
    "    cm_df = pd.DataFrame(confusion_matrix(y_pred, y_true), index=format_labels, columns=format_labels)\n",
    "    cm_df = cm_df.loc[cm_df.sum(axis=1) != 0, :]\n",
    "    cm_df = cm_df.loc[:, cm_df.sum() != 0]\n",
    "    cm_df[\"Row_Sum\"] = cm_df.sum(axis=1)\n",
    "    return cm_df\n",
    "\n",
    "def get_events_dist(attention_query=False):\n",
    "    training_indices_str_org, test_indices_str_org = train_test_split(range(len(events)), test_size=0.2, random_state=42, stratify=events)\n",
    "    context_builder_str, interpreter_str, criterion_str = get_context_builder_interpreter(training_indices_str_org, f'{DATASET}/events')\n",
    "    test_indices_str = get_unique_indices_per_row_or_file(context[test_indices_str_org], f'{DATASET}/events/context_test.pt')\n",
    "    events_test_indices_str   = to_cuda(events [test_indices_str])\n",
    "    context_test_indices_str  = to_cuda(context[test_indices_str])\n",
    "    _, mask = interpreter_str.attended_context(\n",
    "        X           = to_one_hot(context_test_indices_str),\n",
    "        y           = events_test_indices_str.unsqueeze(1),\n",
    "        iterations  = 100 if attention_query else 0\n",
    "    )\n",
    "    targeted_indices = [i for i in range(len(test_indices_str)) if i not in torch.where(~mask)[0].tolist()]\n",
    "    _, perturbed_indices_str, _, _ = get_perturbations_or_location(context_test_indices_str, events_test_indices_str, attention_query=attention_query)\n",
    "    events_series = pd.Series(events_test_indices_str[perturbed_indices_str].cpu()).value_counts().sort_values(ascending=False)\n",
    "    events_df = events_series.reset_index()\n",
    "    events_df.columns = ['label', 'count']\n",
    "    print(len(set(events.tolist())), len(set(events[test_indices_str_org].tolist())), len(set(events_test_indices_str.tolist())))\n",
    "    if labels is not None:\n",
    "        events_df[\"level\"] = events_df[\"label\"].apply(lambda label: list(set(labels[test_indices_str][np.where(events_test_indices_str.cpu() == label)[0]].tolist()))[0])\n",
    "    events_df['print'] = events_df.apply(lambda row: f\"\\\\colorcellpercentamount{{{row['count']}}}{{{torch.sum(events_test_indices_str[targeted_indices] == row['label'])}}}\", axis=1)\n",
    "    events_df.drop(columns=['count'], inplace=True)\n",
    "    if labels is not None:\n",
    "        events_df[\"mapping\"] = events_df[\"label\"].apply(lambda label: mapping[label])\n",
    "    events_df[\"label\"] = events_df[\"label\"].apply(lambda label: f\"\\\\textbf{{{label}}}\")\n",
    "    return events_df\n",
    "\n",
    "def get_events_miss(attention_query=False):\n",
    "    training_indices_str_org, test_indices_str_org = train_test_split(range(len(events)), test_size=0.2, random_state=42, stratify=events)\n",
    "    context_builder_str, interpreter_str, criterion_str = get_context_builder_interpreter(training_indices_str_org, f'{DATASET}/events')\n",
    "    test_indices_str = get_unique_indices_per_row_or_file(context[test_indices_str_org], f'{DATASET}/events/context_test.pt')\n",
    "    events_test_indices_str   = to_cuda(events [test_indices_str])\n",
    "    context_test_indices_str  = to_cuda(context[test_indices_str])\n",
    "    _, mask = interpreter_str.attended_context(\n",
    "        X           = to_one_hot(context_test_indices_str),\n",
    "        y           = events_test_indices_str.unsqueeze(1),\n",
    "        iterations  = 100 if attention_query else 0\n",
    "    )\n",
    "    _, perturbed_indices_str, _, _ = get_perturbations_or_location(context_test_indices_str, events_test_indices_str, attention_query=attention_query)\n",
    "    incorrect_indices = [i for i in range(len(test_indices_str)) if i not in torch.where(~mask)[0].tolist() + perturbed_indices_str.tolist()]\n",
    "    events_series = pd.Series(events_test_indices_str[incorrect_indices].cpu()).value_counts().sort_values(ascending=False)\n",
    "    events_df = events_series.reset_index()\n",
    "    events_df.columns = ['label', 'count']\n",
    "    print(len(set(events.tolist())), len(set(events[test_indices_str_org].tolist())), len(set(events_test_indices_str.tolist())))\n",
    "    if labels is not None:\n",
    "        events_incorrect = events_test_indices_str[incorrect_indices].cpu()\n",
    "        labels_incorrect = labels[incorrect_indices]\n",
    "        events_df[\"level\"] = events_df[\"label\"].apply(lambda label: list(set(labels_incorrect[np.where(events_incorrect == label)[0]].tolist()))[0])\n",
    "\n",
    "    return events_df\n",
    "\n",
    "def show_clusters(x, y):\n",
    "    vectors, mask = interpreter.attended_context(to_one_hot(x), y.reshape(-1, 1))\n",
    "    indices_y = group_by(y[mask].reshape(-1, 1).cpu().numpy(), lambda e: e.data.tobytes(),)\n",
    "    with open(f\"{DATASET}/{SEQ_LEN=}/{PREDICT_THRESHOLD=}/{PICKING}/{MAX_ITER=}/cluster.txt\", \"w\") as file:\n",
    "        for event, context_mask in indices_y:\n",
    "            event = ord(event.decode('ascii')[0])   \n",
    "            if event not in interpreter.tree:\n",
    "                file.write(f\"{\"<\"*50}[{event}]{\">\"*50}\\n\")\n",
    "                continue\n",
    "            file.write(f\"{\"=\"*50}[{event}]{\"=\"*50}\\n\")\n",
    "            vectors_, inverse, _ = sp_unique(vectors[context_mask])\n",
    "            distance, neighbours = interpreter.tree[event].query(\n",
    "                X               = vectors_.toarray(),\n",
    "                return_distance = True,\n",
    "                dualtree        = vectors_.shape[0] >= 1e3, # Optimization\n",
    "            )\n",
    "            neighbours = interpreter.tree[event].get_arrays()[1][neighbours][:, 0]\n",
    "            scores = np.asarray([interpreter.labels[event][neighbour] for neighbour in neighbours])\n",
    "            file.write(format_attention(x[mask], context_mask, vectors, neighbours[inverse], distance[inverse], scores[inverse]) + \"\\n\")"
   ],
   "id": "481027bab25bb665"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T22:57:10.063161Z",
     "start_time": "2024-12-19T22:57:08.476411Z"
    }
   },
   "cell_type": "code",
   "source": "get_events_dist(attention_query=False)",
   "id": "7567607868f2336",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ided/events/no_query/perturbed_collected.pt\n",
      "Loading ided/events/no_query/perturbed_indices.pt\n",
      "Loading ided/events/no_query/perturbed_distribution.pt\n",
      "tensor([ 864, 5050,  140], device='cuda:0')\n",
      "89 84 54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          label  level                                print  \\\n",
       "0   \\textbf{24}      3  \\colorcellpercentamount{2180}{2195}   \n",
       "1   \\textbf{64}      3    \\colorcellpercentamount{847}{889}   \n",
       "2   \\textbf{72}      3    \\colorcellpercentamount{358}{394}   \n",
       "3   \\textbf{66}      3    \\colorcellpercentamount{200}{200}   \n",
       "4   \\textbf{71}      3    \\colorcellpercentamount{173}{173}   \n",
       "5   \\textbf{22}      2    \\colorcellpercentamount{142}{142}   \n",
       "6   \\textbf{86}      3    \\colorcellpercentamount{114}{114}   \n",
       "7   \\textbf{83}      3    \\colorcellpercentamount{109}{109}   \n",
       "8   \\textbf{87}      3    \\colorcellpercentamount{105}{105}   \n",
       "9   \\textbf{20}      3    \\colorcellpercentamount{104}{104}   \n",
       "10  \\textbf{31}      3      \\colorcellpercentamount{95}{95}   \n",
       "11  \\textbf{58}      3      \\colorcellpercentamount{93}{93}   \n",
       "12   \\textbf{2}      2      \\colorcellpercentamount{86}{86}   \n",
       "13  \\textbf{21}      3      \\colorcellpercentamount{67}{72}   \n",
       "14  \\textbf{57}      3      \\colorcellpercentamount{66}{73}   \n",
       "15  \\textbf{15}      2      \\colorcellpercentamount{43}{43}   \n",
       "16  \\textbf{40}      3      \\colorcellpercentamount{42}{42}   \n",
       "17   \\textbf{0}      2      \\colorcellpercentamount{41}{41}   \n",
       "18  \\textbf{42}      3      \\colorcellpercentamount{31}{31}   \n",
       "19  \\textbf{13}      2      \\colorcellpercentamount{22}{22}   \n",
       "20  \\textbf{44}      3      \\colorcellpercentamount{19}{19}   \n",
       "21  \\textbf{78}      3      \\colorcellpercentamount{16}{16}   \n",
       "22   \\textbf{3}      2      \\colorcellpercentamount{15}{15}   \n",
       "23  \\textbf{10}      3      \\colorcellpercentamount{14}{14}   \n",
       "24   \\textbf{9}      2      \\colorcellpercentamount{14}{14}   \n",
       "25  \\textbf{85}      3      \\colorcellpercentamount{11}{18}   \n",
       "26  \\textbf{84}      3      \\colorcellpercentamount{10}{10}   \n",
       "27  \\textbf{79}      3        \\colorcellpercentamount{7}{7}   \n",
       "28  \\textbf{17}      3        \\colorcellpercentamount{7}{7}   \n",
       "29  \\textbf{19}      3        \\colorcellpercentamount{6}{6}   \n",
       "30  \\textbf{74}      3        \\colorcellpercentamount{5}{5}   \n",
       "31  \\textbf{88}      3       \\colorcellpercentamount{4}{32}   \n",
       "32  \\textbf{75}      3        \\colorcellpercentamount{3}{3}   \n",
       "33   \\textbf{1}      2        \\colorcellpercentamount{1}{1}   \n",
       "\n",
       "                                              mapping  \n",
       "0            ET JA3 Hash - [Abuse.ch] Possible Adware  \n",
       "1   SURICATA HTTP unable to match response to request  \n",
       "2         SURICATA STREAM CLOSEWAIT FIN out of window  \n",
       "3      SURICATA Kerberos 5 weak encryption parameters  \n",
       "4   SURICATA STREAM 3way handshake wrong seq wrong...  \n",
       "5                       ET INFO TLS Handshake Failure  \n",
       "6                    SURICATA TLS invalid record type  \n",
       "7                   SURICATA STREAM bad window update  \n",
       "8                 SURICATA TLS invalid record/traffic  \n",
       "9   ET INFO Session Traversal Utilities for NAT (S...  \n",
       "10  ET POLICY GNU/Linux APT User-Agent Outbound li...  \n",
       "11           SURICATA HTTP invalid response chunk len  \n",
       "12                           ET DNS Query for .to TLD  \n",
       "13  ET INFO Session Traversal Utilities for NAT (S...  \n",
       "14            SURICATA HTTP gzip decompression failed  \n",
       "15             ET INFO Observed DNS Query to .biz TLD  \n",
       "16  ET USER_AGENTS Microsoft Device Metadata Retri...  \n",
       "17                           ET DNS Query for .cc TLD  \n",
       "18  SURICATA Applayer Detect protocol only one dir...  \n",
       "19              ET INFO HTTP Request to a *.tw domain  \n",
       "20       SURICATA Applayer Wrong direction first Data  \n",
       "21            SURICATA STREAM FIN1 FIN with wrong seq  \n",
       "22     ET DNS Query to a *.pw domain - Likely Hostile  \n",
       "23  ET INFO External IP Lookup Domain (ipify .org)...  \n",
       "24  ET INFO External IP Lookup Domain (freegeiop ....  \n",
       "25  SURICATA STREAM reassembly overlap with differ...  \n",
       "26          SURICATA STREAM excessive retransmissions  \n",
       "27            SURICATA STREAM Packet with invalid ack  \n",
       "28  ET INFO Observed Discord Domain in DNS Lookup ...  \n",
       "29  ET INFO Session Traversal Utilities for NAT (S...  \n",
       "30            SURICATA STREAM ESTABLISHED invalid ack  \n",
       "31                    SURICATA UDPv6 invalid checksum  \n",
       "32   SURICATA STREAM ESTABLISHED packet out of window  \n",
       "33  ET DNS Query for .su TLD (Soviet Union) Often ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>level</th>\n",
       "      <th>print</th>\n",
       "      <th>mapping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\textbf{24}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{2180}{2195}</td>\n",
       "      <td>ET JA3 Hash - [Abuse.ch] Possible Adware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\textbf{64}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{847}{889}</td>\n",
       "      <td>SURICATA HTTP unable to match response to request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\textbf{72}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{358}{394}</td>\n",
       "      <td>SURICATA STREAM CLOSEWAIT FIN out of window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\textbf{66}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{200}{200}</td>\n",
       "      <td>SURICATA Kerberos 5 weak encryption parameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\textbf{71}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{173}{173}</td>\n",
       "      <td>SURICATA STREAM 3way handshake wrong seq wrong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\textbf{22}</td>\n",
       "      <td>2</td>\n",
       "      <td>\\colorcellpercentamount{142}{142}</td>\n",
       "      <td>ET INFO TLS Handshake Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\textbf{86}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{114}{114}</td>\n",
       "      <td>SURICATA TLS invalid record type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\textbf{83}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{109}{109}</td>\n",
       "      <td>SURICATA STREAM bad window update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\textbf{87}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{105}{105}</td>\n",
       "      <td>SURICATA TLS invalid record/traffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\textbf{20}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{104}{104}</td>\n",
       "      <td>ET INFO Session Traversal Utilities for NAT (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\textbf{31}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{95}{95}</td>\n",
       "      <td>ET POLICY GNU/Linux APT User-Agent Outbound li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\\textbf{58}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{93}{93}</td>\n",
       "      <td>SURICATA HTTP invalid response chunk len</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\\textbf{2}</td>\n",
       "      <td>2</td>\n",
       "      <td>\\colorcellpercentamount{86}{86}</td>\n",
       "      <td>ET DNS Query for .to TLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\\textbf{21}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{67}{72}</td>\n",
       "      <td>ET INFO Session Traversal Utilities for NAT (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\\textbf{57}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{66}{73}</td>\n",
       "      <td>SURICATA HTTP gzip decompression failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\\textbf{15}</td>\n",
       "      <td>2</td>\n",
       "      <td>\\colorcellpercentamount{43}{43}</td>\n",
       "      <td>ET INFO Observed DNS Query to .biz TLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\\textbf{40}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{42}{42}</td>\n",
       "      <td>ET USER_AGENTS Microsoft Device Metadata Retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\\textbf{0}</td>\n",
       "      <td>2</td>\n",
       "      <td>\\colorcellpercentamount{41}{41}</td>\n",
       "      <td>ET DNS Query for .cc TLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\\textbf{42}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{31}{31}</td>\n",
       "      <td>SURICATA Applayer Detect protocol only one dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\\textbf{13}</td>\n",
       "      <td>2</td>\n",
       "      <td>\\colorcellpercentamount{22}{22}</td>\n",
       "      <td>ET INFO HTTP Request to a *.tw domain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\\textbf{44}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{19}{19}</td>\n",
       "      <td>SURICATA Applayer Wrong direction first Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\\textbf{78}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{16}{16}</td>\n",
       "      <td>SURICATA STREAM FIN1 FIN with wrong seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\\textbf{3}</td>\n",
       "      <td>2</td>\n",
       "      <td>\\colorcellpercentamount{15}{15}</td>\n",
       "      <td>ET DNS Query to a *.pw domain - Likely Hostile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\\textbf{10}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{14}{14}</td>\n",
       "      <td>ET INFO External IP Lookup Domain (ipify .org)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\\textbf{9}</td>\n",
       "      <td>2</td>\n",
       "      <td>\\colorcellpercentamount{14}{14}</td>\n",
       "      <td>ET INFO External IP Lookup Domain (freegeiop ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>\\textbf{85}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{11}{18}</td>\n",
       "      <td>SURICATA STREAM reassembly overlap with differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\\textbf{84}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{10}{10}</td>\n",
       "      <td>SURICATA STREAM excessive retransmissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\\textbf{79}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{7}{7}</td>\n",
       "      <td>SURICATA STREAM Packet with invalid ack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>\\textbf{17}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{7}{7}</td>\n",
       "      <td>ET INFO Observed Discord Domain in DNS Lookup ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\\textbf{19}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{6}{6}</td>\n",
       "      <td>ET INFO Session Traversal Utilities for NAT (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>\\textbf{74}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{5}{5}</td>\n",
       "      <td>SURICATA STREAM ESTABLISHED invalid ack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>\\textbf{88}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{4}{32}</td>\n",
       "      <td>SURICATA UDPv6 invalid checksum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>\\textbf{75}</td>\n",
       "      <td>3</td>\n",
       "      <td>\\colorcellpercentamount{3}{3}</td>\n",
       "      <td>SURICATA STREAM ESTABLISHED packet out of window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>\\textbf{1}</td>\n",
       "      <td>2</td>\n",
       "      <td>\\colorcellpercentamount{1}{1}</td>\n",
       "      <td>ET DNS Query for .su TLD (Soviet Union) Often ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Without Attention Query",
   "id": "227b330b3da77190"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:30:04.063102Z",
     "start_time": "2024-12-17T12:30:04.053402Z"
    }
   },
   "cell_type": "code",
   "source": "perturbed_collected, perturbed_indices, perturb_distribution, perturbed_iterations = get_perturbations_or_file(context_test, events_test)",
   "id": "7505db07e1f3f663",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hdfs/SEQ_LEN=10/PREDICT_THRESHOLD=0.2/rand/no_query/perturbed_collected.pt\n",
      "Loading hdfs/SEQ_LEN=10/PREDICT_THRESHOLD=0.2/rand/no_query/perturbed_indices.pt\n",
      "Loading hdfs/SEQ_LEN=10/PREDICT_THRESHOLD=0.2/rand/no_query/perturbed_distribution.pt\n",
      "tensor([ 591, 4743,   69], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:30:04.072354Z",
     "start_time": "2024-12-17T12:30:04.065732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "perturbed_minimized = get_shortcuts_or_file(\n",
    "    perturbed_collected, \n",
    "    context_test[perturbed_indices], \n",
    "    events_test[perturbed_indices]\n",
    ")"
   ],
   "id": "6275be159cfa6735",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hdfs/SEQ_LEN=10/PREDICT_THRESHOLD=0.2/rand/no_query/shortcuts.pt\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:30:04.083955Z",
     "start_time": "2024-12-17T12:30:04.074331Z"
    }
   },
   "cell_type": "code",
   "source": "format_series(perturbed_iterations.cpu())",
   "id": "3c5b79ab252ef524",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     3447\n",
       "2      779\n",
       "3      279\n",
       "4       96\n",
       "5       67\n",
       "6       24\n",
       "7       16\n",
       "8       10\n",
       "9        6\n",
       "10       5\n",
       "11       4\n",
       "12       4\n",
       "13       1\n",
       "14       1\n",
       "15       3\n",
       "18       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:30:04.328821Z",
     "start_time": "2024-12-17T12:30:04.085838Z"
    }
   },
   "cell_type": "code",
   "source": "format_confusion_matrix(labels_test[perturbed_indices].cpu(), interpret(context_test[perturbed_indices], events_test[perturbed_indices]))",
   "id": "1e91ea3cf1e14ab9",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m format_confusion_matrix(\u001B[43mlabels_test\u001B[49m\u001B[43m[\u001B[49m\u001B[43mperturbed_indices\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mcpu(), interpret(context_test[perturbed_indices], events_test[perturbed_indices]))\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_matrix_perturb(context_test[perturbed_indices], perturbed_minimized)",
   "id": "17324785b622310e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "format_confusion_matrix(labels_test.cpu(), get_combined(perturbed_minimized, perturbed_indices))",
   "id": "7d88317f2f69085",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Applying No Attention Query Data to Attention Query",
   "id": "916c4370e957c843"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "format_confusion_matrix(labels_test.cpu(), get_combined(perturbed_minimized, perturbed_indices, attention_query=True))",
   "id": "bca267d206346dc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention Query",
   "id": "dd7d7f51a223e16d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "perturbed_collected_query, perturbed_indices_query, perturb_distribution_query, perturbed_iterations_query = get_perturbations_or_file(context_test, events_test, attention_query=True)",
   "id": "d9c4fa7234b074ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "format_series(events_test[perturbed_indices_query].cpu())",
   "id": "70406875b1b4c57a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "perturbed_minimized_query = get_shortcuts_or_file(\n",
    "    perturbed_collected_query,\n",
    "    context_test[perturbed_indices_query], \n",
    "    events_test[perturbed_indices_query],\n",
    "    attention_query=True\n",
    ")"
   ],
   "id": "fe1f04a9a509374b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "format_series(perturbed_iterations_query.cpu())",
   "id": "f8e17399ea78aaec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_matrix_perturb(context_test[perturbed_indices_query], perturbed_minimized_query)",
   "id": "97f3dbe019df757d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "format_confusion_matrix(labels_test[perturbed_indices_query].cpu(), interpret_query(context_test[perturbed_indices_query], events_test[perturbed_indices_query]))",
   "id": "4af53cca9dd7c0ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "format_confusion_matrix(labels_test.cpu(), get_combined(perturbed_minimized_query, perturbed_indices_query, attention_query=True))",
   "id": "70f00f41b82b98a9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
